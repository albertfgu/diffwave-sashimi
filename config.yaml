diffusion:
  T: 200
  beta_0: 0.0001
  beta_T: 0.02
  beta: null

model:
  in_channels: 1
  out_channels: 1
  diffusion_step_embed_dim_in: 128
  diffusion_step_embed_dim_mid: 512
  diffusion_step_embed_dim_out: 512
  res_channels: 128
  skip_channels: 256
  num_res_layers: 30
  dilation_cycle: 10
  sashimi: true
  unet: true
  d_model: 64
  n_layers: 6
  pool: [4, 4]
  expand: 2
  ff: 2
  unconditional: true

dataset:
  data_path: data/sc09
  segment_length: 16000
  sampling_rate: 16000

train: # Not used in generate.py
  # stdout_dir: exp/ # directory to save stdout logs
  name: null # Name of experiment (prefix of experiment name)
  ckpt_iter: max
  iters_per_ckpt: 10000
  iters_per_logging: 100
  n_iters: 1000001
  learning_rate: 2e-4
  batch_size_per_gpu: 4
  n_samples: 32 # Samples per GPU to generate per checkpoint
  mel_path: null # Path to load spectrograms from for vocoding

generate: # Not used in train.py
  name: null # Name of experiment (prefix of experiment name)
  ckpt_iter: max # Which checkpoint to use; assign a number or "max"
  ckpt_smooth: -1 # Which checkpoint to start averaging from (experimental)
  n_samples: 16 # Number of utterances to be generated (per GPU)
  batch_size: null # Number of samples to generate at once per GPU. null means max (equal to samples_per_gpu)

distributed:
  dist_backend: nccl
  dist_url: tcp://localhost:54321

wandb:
  mode: disabled # Pass in 'wandb.mode=online' to turn on wandb logging
  project: hippo
  entity: null
  id: null # Set to string to resume logging from run
  job_type: training

